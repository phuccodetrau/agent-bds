{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import underthesea\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import col, udf, lit, greatest, monotonically_increasing_id, concat_ws\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, StructType, StructField, MapType, BooleanType\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/03 18:51:59 WARN Utils: Your hostname, haihp02 resolves to a loopback address: 127.0.1.1; using 192.168.102.7 instead (on interface wlp5s0)\n",
      "24/01/03 18:51:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/03 18:52:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/03 18:52:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.builder.master(\"spark://172.24.0.11:7077\").getOrCreate()\n",
    "spark = SparkSession.builder.appName('streaming').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hdfs_file_path = [\n",
    "    'hdfs://192.168.102.7:10000/haihp02/real_estate_data/bds.jsonl',\n",
    "    'hdfs://192.168.102.7:10000/haihp02/real_estate_data/i-batdongsan.jsonl',\n",
    "    'hdfs://192.168.102.7:10000/haihp02/real_estate_data/nhadatviet.jsonl',\n",
    "]\n",
    "df = spark.read.json(hdfs_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- district: string (nullable = true)\n",
      " |    |-- full_address: string (nullable = true)\n",
      " |    |-- province: string (nullable = true)\n",
      " |    |-- ward: string (nullable = true)\n",
      " |-- contact_info: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- phone: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- estate_type: string (nullable = true)\n",
      " |-- extra_infos: struct (nullable = true)\n",
      " |    |-- Chi·ªÅu d√†i: string (nullable = true)\n",
      " |    |-- Chi·ªÅu ngang: string (nullable = true)\n",
      " |    |-- Ch√≠nh ch·ªß: boolean (nullable = true)\n",
      " |    |-- Ch·ªï ƒë·ªÉ xe h∆°i: boolean (nullable = true)\n",
      " |    |-- H∆∞·ªõng: string (nullable = true)\n",
      " |    |-- Lo·∫°i tin: string (nullable = true)\n",
      " |    |-- L·ªô gi·ªõi: string (nullable = true)\n",
      " |    |-- Nh√† b·∫øp: boolean (nullable = true)\n",
      " |    |-- Ph√°p l√Ω: string (nullable = true)\n",
      " |    |-- Ph√≤ng ƒÉn: boolean (nullable = true)\n",
      " |    |-- S√¢n th∆∞·ª£ng: boolean (nullable = true)\n",
      " |    |-- S·ªë l·∫ßu: string (nullable = true)\n",
      " |    |-- S·ªë ph√≤ng ng·ªß: string (nullable = true)\n",
      " |    |-- S·ªë ph√≤ng ng·ªß :: string (nullable = true)\n",
      " |    |-- S·ªë toilet :: string (nullable = true)\n",
      " |    |-- T·∫ßng :: string (nullable = true)\n",
      " |-- link: string (nullable = true)\n",
      " |-- post_date: string (nullable = true)\n",
      " |-- post_id: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- square: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "250796"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De-duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"text\", concat_ws(' ', col('title'), col('description')))\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "df = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"tf\")\n",
    "df = hashingTF.transform(df)\n",
    "\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "idf_model = idf.fit(df)\n",
    "df = idf_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_non_zero_udf = udf(lambda v: Vectors.sparse(len(v) + 1, list(v.indices) + [len(v)], list(v.values) + [1e-5]), VectorUDT())\n",
    "\n",
    "df = df.withColumn(\"tfidf\", append_non_zero_udf(col(\"tfidf\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 22:48:16 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n"
     ]
    }
   ],
   "source": [
    "num_hash_tables = 5\n",
    "minhashLSH = MinHashLSH(inputCol=\"tfidf\", outputCol=\"hashes\", numHashTables=num_hash_tables)\n",
    "model = minhashLSH.fit(df)\n",
    "df_duplicates = model.approxSimilarityJoin(df.select(\"Id\", \"tfidf\"), df.select(\"Id\", \"tfidf\"), 0.8, distCol=\"JaccardDistance\") \\\n",
    "    .filter(\"datasetA.id < datasetB.id\")  # Avoid comparing a row to itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 22:48:17 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/01/02 22:48:18 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|            datasetA|            datasetB|   JaccardDistance|\n",
      "+--------------------+--------------------+------------------+\n",
      "|{6, (262145,[191,...|{16, (262145,[191...|0.7549019607843137|\n",
      "|{6, (262145,[191,...|{14, (262145,[191...|0.6513761467889908|\n",
      "|{13, (262145,[7,4...|{15, (262145,[191...|0.7669902912621359|\n",
      "|{11, (262145,[191...|{12, (262145,[704...|0.7272727272727273|\n",
      "|{3, (262145,[191,...|{18, (262145,[191...|0.5189873417721519|\n",
      "|{2, (262145,[191,...|{18, (262145,[191...|0.7213114754098361|\n",
      "|{3, (262145,[191,...|{6, (262145,[191,...|0.5544554455445545|\n",
      "|{14, (262145,[191...|{16, (262145,[191...|0.6578947368421053|\n",
      "|{2, (262145,[191,...|{15, (262145,[191...|0.7355371900826446|\n",
      "|{16, (262145,[191...|{18, (262145,[191...|0.7272727272727273|\n",
      "|{4, (262145,[191,...|{13, (262145,[7,4...|0.7966101694915254|\n",
      "|{5, (262145,[191,...|{13, (262145,[7,4...|0.7699115044247787|\n",
      "|{6, (262145,[191,...|{17, (262145,[191...|0.6388888888888888|\n",
      "|{6, (262145,[191,...|{18, (262145,[191...|0.6116504854368932|\n",
      "|{18, (262145,[191...|{19, (262145,[191...|              0.36|\n",
      "|{0, (262145,[191,...|{3, (262145,[191,...|0.6989247311827957|\n",
      "|{0, (262145,[191,...|{17, (262145,[191...|0.6444444444444444|\n",
      "|{4, (262145,[191,...|{16, (262145,[191...| 0.735632183908046|\n",
      "|{10, (262145,[191...|{12, (262145,[704...|0.7948717948717949|\n",
      "|{1, (262145,[191,...|{7, (262145,[191,...|0.6206896551724138|\n",
      "+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 22:48:19 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    }
   ],
   "source": [
    "df_duplicates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = df.join(remove_ids, df[\"Id\"] == remove_ids[\"id\"], \"leftanti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 22:53:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/01/02 22:53:33 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "24/01/02 22:53:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------+--------------------+--------------------+----------+-------+----------+------+--------------------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|             address|        contact_info|         description| estate_type|         extra_infos|                link| post_date|post_id|     price|square|               title| Id|                text|              tokens|                  tf|               tfidf|\n",
      "+--------------------+--------------------+--------------------+------------+--------------------+--------------------+----------+-------+----------+------+--------------------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|{Hai B√† Tr∆∞ng, ƒë∆∞...|{L·∫°i Ti·∫øn D≈©ng, [...|D√≤ng ti·ªÅn ·ªïn ƒë·ªãnh...|Nh√† ng√µ, h·∫ªm|{10.7m, 4.2m, ƒê√¥n...|https://guland.vn...|2023/05/29| 464352|5500000000|    45|D√≤ng ti·ªÅn ·ªïn ƒë·ªãnh...|  7|D√≤ng ti·ªÅn ·ªïn ƒë·ªãnh...|[d√≤ng, ti·ªÅn, ·ªïn, ...|(262144,[191,1344...|(262145,[191,1344...|\n",
      "|{Hai B√† Tr∆∞ng, ƒë∆∞...|{Ng√¥ Th·ªã Thu·ª∑, [0...|B√°n nh√† ng√µ 191 M...|Nh√† ng√µ, h·∫ªm|{10m, 3.6m, Nam, ...|https://guland.vn...|2023/05/29| 463917|3450000000|    36|B√°n nh√† ng√µ 191 M...| 19|B√°n nh√† ng√µ 191 M...|[b√°n, nh√†, ng√µ, 1...|(262144,[191,8808...|(262145,[191,8808...|\n",
      "+--------------------+--------------------+--------------------+------------+--------------------+--------------------+----------+-------+----------+------+--------------------+---+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/02 22:53:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_chars(df: pyspark.sql.dataframe.DataFrame):\n",
    "    # get concatenated text\n",
    "    concatenated_text = df.select(f.concat_ws(' ', col('title'), col('description')).alias('concatenated_text'))\n",
    "    all_characters = concatenated_text.rdd.flatMap(lambda x: x[0])\n",
    "    special_characters = all_characters.filter(lambda c: not c.isalnum() and not c.isspace() and not c in string.punctuation)\n",
    "    return set(special_characters.collect())\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def remove_special_chars(input_string, special_chars_list, at_once=False):\n",
    "    if not input_string:\n",
    "        return None\n",
    "    if at_once:\n",
    "        special_chars_string = ''.join(special_chars_list)\n",
    "        translator = str.maketrans('', '', special_chars_string)\n",
    "        result = input_string.translate(translator)\n",
    "    else:\n",
    "        result = input_string\n",
    "        for c in special_chars_list:\n",
    "            result = result.replace(c, '')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "special_chars_list = list(get_special_chars(df))\n",
    "special_chars_lit = lit(special_chars_list)\n",
    "df = df.withColumn(\"title\", remove_special_chars(\"title\", special_chars_lit))\n",
    "df = df.withColumn(\"description\", remove_special_chars(\"description\", special_chars_lit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚Üí',\n",
       " '\\u202a',\n",
       " '\\uf0d8',\n",
       " '‚ú§',\n",
       " '\\u200c',\n",
       " '€£',\n",
       " 'üÖñ',\n",
       " '‚Äì',\n",
       " '‚Çã',\n",
       " '‚óè',\n",
       " '¬¨',\n",
       " 'Ã∂',\n",
       " '‚ñ¨',\n",
       " '‚âà',\n",
       " 'ü´µ',\n",
       " '‚óá',\n",
       " '‚ñ∑',\n",
       " 'ü™∑',\n",
       " '‚óä',\n",
       " '‚Äê',\n",
       " 'ü´¥',\n",
       " '\\uf05b',\n",
       " '‚¶Å',\n",
       " 'Ô∏è',\n",
       " '„é°',\n",
       " 'ü´∞',\n",
       " '‚Ä≤',\n",
       " '‚ú•',\n",
       " '‚úß',\n",
       " '‚ô§',\n",
       " 'ü´∂',\n",
       " '€ú',\n",
       " '‚ùÉ',\n",
       " 'ÃÄ',\n",
       " '÷ç',\n",
       " '\\u2060',\n",
       " '\\u206e',\n",
       " '‚Äò',\n",
       " '‚ùà',\n",
       " 'üÖ£',\n",
       " 'üÖò',\n",
       " '‚ÑÖ',\n",
       " '\\ufeff',\n",
       " '‚Ä≥',\n",
       " '\\u200b',\n",
       " '‚ôö',\n",
       " 'Ã£',\n",
       " '‚Ç´',\n",
       " '\\uf06e',\n",
       " '‚ú©',\n",
       " 'üÖ®',\n",
       " '‚Äô',\n",
       " '\\xad',\n",
       " '‚òÖ',\n",
       " '¬±',\n",
       " '\\U0001fae8',\n",
       " 'Ô∏é',\n",
       " '\\uf0f0',\n",
       " '‚àô',\n",
       " '‚ôõ',\n",
       " 'Ãâ',\n",
       " 'Ãõ',\n",
       " '‚ùÜ',\n",
       " '‚úú',\n",
       " '√∑',\n",
       " '‚ôú',\n",
       " '¬∑',\n",
       " '‚ùñ',\n",
       " '„Äë',\n",
       " '‚ùÅ',\n",
       " 'ü´±',\n",
       " '„Éª',\n",
       " '‚Ç¨',\n",
       " '‚òõ',\n",
       " '‚Äú',\n",
       " '‚ñ†',\n",
       " '\\uf046',\n",
       " 'Ôøº',\n",
       " 'ÔøΩ',\n",
       " '\\u200d',\n",
       " 'ü´†',\n",
       " '\\uf0e8',\n",
       " '‚ÅÉ',\n",
       " '‚â•',\n",
       " 'ÔΩû',\n",
       " '‚û£',\n",
       " 'ÃÅ',\n",
       " 'ü™©',\n",
       " 'ÃÉ',\n",
       " '\\uf02b',\n",
       " '·™•',\n",
       " 'ü™∫',\n",
       " '‚ôß',\n",
       " '‚ùÇ',\n",
       " '„ÄÇ',\n",
       " '‚ô°',\n",
       " 'Ôºå',\n",
       " 'ü™∏',\n",
       " 'Ôºö',\n",
       " '¬•',\n",
       " '‚ùù',\n",
       " 'ÃÇ',\n",
       " '\\U0001fa77',\n",
       " '\\uf0a7',\n",
       " '‡ß£',\n",
       " '‚öò',\n",
       " '‚û¢',\n",
       " '‚áî',\n",
       " '„ÄÅ',\n",
       " 'Ôºç',\n",
       " '‚úÜ',\n",
       " 'ü´£',\n",
       " '‚õ´',\n",
       " '‚ñ∫',\n",
       " 'ÃÜ',\n",
       " '‚úé',\n",
       " '‚ùØ',\n",
       " '„Ää',\n",
       " '\\uf076',\n",
       " '‚ùÆ',\n",
       " '‚ùÄ',\n",
       " 'Ãµ',\n",
       " 'ü•π',\n",
       " '‚ùâ',\n",
       " 'Ã∑',\n",
       " '\\uf028',\n",
       " '‚úΩ',\n",
       " '¬´',\n",
       " '‚áí',\n",
       " '‚û§',\n",
       " '\\uf0e0',\n",
       " '\\U0001faad',\n",
       " '‚ôô',\n",
       " '\\uf0fc',\n",
       " '„Äê',\n",
       " '‚û•',\n",
       " '¬§',\n",
       " 'ÔºÜ',\n",
       " 'üõá',\n",
       " '\\x7f',\n",
       " 'Ôºâ',\n",
       " '‚Äî',\n",
       " '‚Äù',\n",
       " '‚ùû',\n",
       " '„Äã',\n",
       " '‚òÜ',\n",
       " '√ó',\n",
       " '‚úû',\n",
       " '‚úø',\n",
       " '‚â§',\n",
       " 'üÖê',\n",
       " '‚àö',\n",
       " '¬∞',\n",
       " '‚úì',\n",
       " '¬°',\n",
       " '‚Ä¶',\n",
       " '‚Ä¢',\n",
       " '¬ª',\n",
       " '‚ùä',\n",
       " '‚û¶',\n",
       " '\\u06dd',\n",
       " '\\uf06c',\n",
       " '¬∏']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_chars_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def remove_duplicate_punctuation_sequence(input_string):\n",
    "    def remove_duplicate_sequence(text, target_char, max_length):\n",
    "        pattern_1 = re.escape(target_char) + '{' + str(max_length) + ',}'\n",
    "        pattern_2 = '(' + '\\s' + re.escape(target_char) + ')' + '{' + str(max_length) + ',}'\n",
    "        result = re.sub(pattern_2, target_char, re.sub(pattern_1, target_char, text))\n",
    "        return result\n",
    "    \n",
    "    if not input_string:\n",
    "        return None\n",
    "    result = input_string\n",
    "    for punc in string.punctuation:\n",
    "        if punc == '\\\\':\n",
    "            continue\n",
    "        max_length = 3 if punc == '.' else 1\n",
    "        reuslt = remove_duplicate_sequence(result, punc, max_length)\n",
    "    return reuslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"title\", remove_duplicate_punctuation_sequence(\"title\"))\n",
    "df = df.withColumn(\"description\", remove_duplicate_punctuation_sequence(\"description\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estate_types(df: pyspark.sql.dataframe.DataFrame):\n",
    "    df = df.filter(df['estate_type'].isNotNull())\n",
    "    all_estate_types = df.select('estate_type').rdd.map(lambda x: x[0])\n",
    "    estate_types_set = set(all_estate_types.collect())\n",
    "    return estate_types_set\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def normalize_estate_type(input_estate_type):\n",
    "    if not input_estate_type:\n",
    "        return None\n",
    "    estate_type_prefix = ['Cho thu·ªÉ', 'Mua b√°n', 'CƒÉn h·ªô']\n",
    "    estate_type_map = {\n",
    "        'Bi·ªát th·ª±, li·ªÅn k`·ªÅ': 'Bi·ªát th·ª± li·ªÅn k·ªÅ',\n",
    "        'Nh√† bi·ªát th·ª± li·ªÅn k·ªÅ': 'Bi·ªát th·ª± li·ªÅn k·ªÅ',\n",
    "        'Nh√† m·∫∑t ph·ªë': 'Nh√† m·∫∑t ti·ªÅn',\n",
    "        'Ph√≤ng tr·ªç, nh√† tr·ªç, nh√† tr·ªç': 'Ph√≤ng tr·ªç, nh√† tr·ªç',\n",
    "        'Ph√≤ng tr·ªç': 'Ph√≤ng tr·ªç, nh√† tr·ªç',\n",
    "        'Trang tr·∫°i, khu ngh·ªâ d∆∞·ª°ng': 'Trang tr·∫°i khu ngh·ªâ d∆∞·ª°ng',\n",
    "        'Kho nh√† x∆∞·ªüng': 'Kho x∆∞·ªüng',\n",
    "        'Kho, x∆∞·ªüng': 'Kho x∆∞·ªüng'\n",
    "    }\n",
    "    result = input_estate_type\n",
    "    for prefix in estate_type_prefix:\n",
    "        result = result.replace(prefix, '').strip().capitalize()\n",
    "    for estate_type in estate_type_map.keys():\n",
    "        if result == estate_type:\n",
    "            result = estate_type_map[estate_type]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"estate_type\", normalize_estate_type(\"estate_type\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=FloatType())\n",
    "def price_normalize(price, square):\n",
    "    if price is None:\n",
    "        return None\n",
    "    if isinstance(price, int) or isinstance(price, float):\n",
    "        return price\n",
    "    elif isinstance(price, str):\n",
    "        if price.isnumeric():\n",
    "            return float(price)\n",
    "        if square is not None:\n",
    "            price = underthesea.text_normalize(price)\n",
    "            # C√°c tr∆∞·ªùng h·ª£p th·ª±c s·ª± ƒëi·ªÅn gi√° / m2\n",
    "            if 'tri·ªáu/ m' in price or 'tri·ªáu / m' in price:\n",
    "                price = float(price.split()[0]) * 1e6 * square\n",
    "            # C√°c tr∆∞·ªùng h·ª£p ƒëi·ªÅn nh·∫ßm gi√° sang gi√° / m2\n",
    "            elif 't·ª∑/ m' in price or 't·ª∑ / m' in price:\n",
    "                price = float(price.split()[0]) * 1e9\n",
    "            else:\n",
    "                price = None\n",
    "        elif square is None:\n",
    "            price = None\n",
    "    return price\n",
    "\n",
    "def get_lower_upper_bound(df, col_name, lower_percent=5, upper_percent=95, outlier_threshold=5):\n",
    "    lower_percentile, upper_percentile = df.approxQuantile(col_name, [lower_percent/100, upper_percent/100], 0.01)\n",
    "    quantile_range = upper_percentile - lower_percentile\n",
    "    lower_bound = np.max([0, lower_percentile - outlier_threshold * quantile_range])\n",
    "    upper_bound = upper_percentile + outlier_threshold * quantile_range\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "def get_detail_lower_upper_bound(df, col_name, lower_percent=5, upper_percent=95, outlier_threshold=5, overprice=1e15):\n",
    "    quantiles_by_estate_type = (\n",
    "        df.groupBy(\"estate_type\")\n",
    "        .agg(f.percentile_approx(col_name, [lower_percent/100, upper_percent/100], 100).alias(\"percentile_approx\"))\n",
    "    )\n",
    "    quantiles_by_estate_type = quantiles_by_estate_type.withColumn(\"lower_percentile\", col(\"percentile_approx\").getItem(0)) \\\n",
    "                                                       .withColumn(\"upper_percentile\", col(\"percentile_approx\").getItem(1)) \\\n",
    "                                                       .withColumn(\"quantile_range\", col(\"upper_percentile\") - col(\"lower_percentile\"))\n",
    "    quantiles_by_estate_type = quantiles_by_estate_type.withColumn(\"lower_bound\", greatest(col(\"lower_percentile\") - outlier_threshold * col(\"quantile_range\"), lit(0))) \\\n",
    "                                                       .withColumn(\"upper_bound\", col(\"upper_percentile\") + outlier_threshold * col(\"quantile_range\"))\n",
    "    \n",
    "    return quantiles_by_estate_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+----------------+--------------+-----------+-----------+\n",
      "|         estate_type|   percentile_approx|lower_percentile|upper_percentile|quantile_range|lower_bound|upper_bound|\n",
      "+--------------------+--------------------+----------------+----------------+--------------+-----------+-----------+\n",
      "|             Nh√† ph·ªë|    [4.0E9, 1.22E11]|           4.0E9|         1.22E11|       1.18E11|        0.0|    7.12E11|\n",
      "|           VƒÉn ph√≤ng|    [7.8E9, 3.83E11]|           7.8E9|         3.83E11|      3.752E11|        0.0|   2.259E12|\n",
      "|ƒê·∫•t n√¥ng, l√¢m nghi·ªáp|[3000000.0, 1.125...|       3000000.0|        1.125E11|    1.12497E11|        0.0| 6.74985E11|\n",
      "|  M·∫∑t b·∫±ng, c·ª≠a h√†ng|     [2.0E7, 1.2E11]|           2.0E7|          1.2E11|     1.1998E11|        0.0|   7.199E11|\n",
      "|   Bi·ªát th·ª±, li·ªÅn k·ªÅ|     [6.3E9, 8.0E10]|           6.3E9|          8.0E10|       7.37E10|        0.0|   4.485E11|\n",
      "|    ƒê·∫•t n·ªÅn, ph√¢n l√¥|     [5.0E8, 3.7E10]|           5.0E8|          3.7E10|       3.65E10|        0.0|   2.195E11|\n",
      "|           Ph√≤ng tr·ªç|    [5.7E9, 2.59E10]|           5.7E9|         2.59E10|       2.02E10|        0.0|   1.269E11|\n",
      "| Nh√† h√†ng, kh√°ch s·∫°n|    [2.5E8, 2.81E11]|           2.5E8|         2.81E11|     2.8075E11|        0.0| 1.68475E12|\n",
      "|Trang tr·∫°i, khu n...|     [7.9E8, 6.0E10]|           7.9E8|          6.0E10|      5.921E10|        0.0|  3.5605E11|\n",
      "|       C√°c lo·∫°i kh√°c|    [1.08E8, 1.3E11]|          1.08E8|          1.3E11|    1.29892E11|        0.0|  7.7946E11|\n",
      "|           Nh√† x∆∞·ªüng|     [9.6E7, 2.5E11]|           9.6E7|          2.5E11|    2.49904E11|        0.0| 1.49952E12|\n",
      "|            Chung c∆∞|     [9.85E8, 7.4E9]|          9.85E8|           7.4E9|       6.415E9|        0.0|  3.9475E10|\n",
      "|           Nh√† ri√™ng|    [2.4E9, 1.85E10]|           2.4E9|         1.85E10|       1.61E10|        0.0|     9.9E10|\n",
      "|  Ph√≤ng tr·ªç, nh√† tr·ªç|     [5.7E9, 2.7E10]|           5.7E9|          2.7E10|       2.13E10|        0.0|   1.335E11|\n",
      "|        Nh√† m·∫∑t ti·ªÅn|    [3.35E9, 1.0E11]|          3.35E9|          1.0E11|      9.665E10|        0.0|  5.8325E11|\n",
      "|            Bi·ªát th·ª±|     [6.5E9, 9.5E10]|           6.5E9|          9.5E10|       8.85E10|        0.0|   5.375E11|\n",
      "|          Kho, x∆∞·ªüng|     [7.5E7, 2.5E11]|           7.5E7|          2.5E11|    2.49925E11|        0.0|1.499625E12|\n",
      "|                 ƒê·∫•t|    [5.34E8, 3.3E10]|          5.34E8|          3.3E10|     3.2466E10|        0.0|  1.9533E11|\n",
      "|    Shop, kiot, qu√°n|     [5.8E7, 5.0E10]|           5.8E7|          5.0E10|     4.9942E10|        0.0|  2.9971E11|\n",
      "|          Trang tr·∫°i|  [2222000.0, 6.7E9]|       2222000.0|           6.7E9|    6.697778E9|        0.0|4.018889E10|\n",
      "+--------------------+--------------------+----------------+----------------+--------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "get_detail_lower_upper_bound(df, \"price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extra_info_labels(df):\n",
    "    extra_infos_df = df.select(\"extra_infos\")\n",
    "    extra_infos_labels = extra_infos_df.rdd.flatMap(lambda x: list(x[0].asDict().keys())).collect()\n",
    "    return set(extra_infos_labels)\n",
    "\n",
    "def cast_to_string(value):\n",
    "    try:\n",
    "        return str(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def cast_to_boolean(value):\n",
    "    try:\n",
    "        return bool(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "def cast_to_integer(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "    \n",
    "def cast_to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "    \n",
    "def normalize_text_field_in_dict(dict_obj):\n",
    "    result_dict = dict_obj\n",
    "    for key in result_dict.keys():\n",
    "        if isinstance(result_dict[key], str):\n",
    "            result_dict[key] = result_dict[key].replace(',', '.')\n",
    "            new_val = ''\n",
    "            for c in result_dict[key]:\n",
    "                if c.isalpha() or c.isnumeric() or c == '.' or c == ' ':\n",
    "                    new_val += c\n",
    "            result_dict[key] = new_val\n",
    "    return result_dict\n",
    "\n",
    "old_keys = ['Chi·ªÅu d√†i', 'Chi·ªÅu ngang', 'Ch√≠nh ch·ªß', 'Ch·ªï ƒë·ªÉ xe h∆°i', 'H∆∞·ªõng', 'Lo·∫°i tin', 'L·ªô gi·ªõi', 'Nh√† b·∫øp', 'Ph√°p l√Ω', 'Ph√≤ng ƒÉn', 'S√¢n th∆∞·ª£ng', 'S·ªë l·∫ßu', 'S·ªë ph√≤ng ng·ªß', 'S·ªë ph√≤ng ng·ªß :', 'S·ªë toilet :', 'T·∫ßng :']\n",
    "new_keys = ['Chi·ªÅu d√†i', 'Chi·ªÅu ngang', 'Ch√≠nh ch·ªß', 'Ch·ªó ƒë·ªÉ xe h∆°i', 'H∆∞·ªõng', 'remove', 'L·ªô gi·ªõi', 'Nh√† b·∫øp', 'Ph√°p l√Ω', 'Ph√≤ng ƒÉn', 'S√¢n th∆∞·ª£ng', 'S·ªë l·∫ßu', 'S·ªë ph√≤ng ng·ªß', 'S·ªë ph√≤ng ng·ªß', 'S·ªë toilet', 'T·∫ßng']\n",
    "remove_keys = ['remove']\n",
    "@udf(returnType=StructType([\n",
    "    StructField('Chi·ªÅu d√†i', FloatType()),\n",
    "    StructField('Chi·ªÅu ngang', FloatType()),\n",
    "    StructField('Ch√≠nh ch·ªß', BooleanType()),\n",
    "    StructField('Ch·ªó ƒë·ªÉ xe h∆°i', BooleanType()),\n",
    "    StructField('H∆∞·ªõng', StringType()),\n",
    "    StructField('L·ªô gi·ªõi', FloatType()),\n",
    "    StructField('Nh√† b·∫øp', BooleanType()),\n",
    "    StructField('Ph√°p l√Ω', StringType()),\n",
    "    StructField('Ph√≤ng ƒÉn', BooleanType()),\n",
    "    StructField('S√¢n th∆∞·ª£ng', BooleanType()),\n",
    "    StructField('S·ªë l·∫ßu', IntegerType()),\n",
    "    StructField('S·ªë ph√≤ng ng·ªß', IntegerType()),\n",
    "    StructField('S·ªë toilet', IntegerType()),\n",
    "    StructField('T·∫ßng', IntegerType()),\n",
    "]))\n",
    "def normalize_extra_infos_dict(input_extra_infos_row, old_keys, new_keys, remove_keys):\n",
    "    old_keys = list(old_keys)\n",
    "    new_keys = list(new_keys)\n",
    "    remove_keys = list(remove_keys)\n",
    "    assert len(old_keys) == len(new_keys)\n",
    "\n",
    "    # Normalize dict keys\n",
    "    extra_infos_dict = input_extra_infos_row.asDict()\n",
    "    dict_nomalized_keys = {}\n",
    "\n",
    "    for old_key, new_key in zip(old_keys, new_keys):\n",
    "        if old_key in extra_infos_dict.keys():\n",
    "            if new_key in dict_nomalized_keys.keys() and dict_nomalized_keys[new_key] is None \\\n",
    "                or new_key not in dict_nomalized_keys.keys():\n",
    "                dict_nomalized_keys[new_key] = extra_infos_dict[old_key]\n",
    "        else:\n",
    "            dict_nomalized_keys[new_key] = None\n",
    "    for key in remove_keys:\n",
    "        if key in dict_nomalized_keys.keys():\n",
    "            dict_nomalized_keys.pop(key)\n",
    "    # Normalize dict values\n",
    "    result_dict = normalize_text_field_in_dict(dict_nomalized_keys)\n",
    "    result_dict['Chi·ªÅu d√†i'] = cast_to_float(dict_nomalized_keys['Chi·ªÅu d√†i'].replace('m', ''))\n",
    "    result_dict['Chi·ªÅu ngang'] = cast_to_float(dict_nomalized_keys['Chi·ªÅu ngang'].replace('m', ''))\n",
    "    result_dict['Ch√≠nh ch·ªß'] = cast_to_boolean(dict_nomalized_keys['Ch√≠nh ch·ªß'])\n",
    "    result_dict['Ch·ªó ƒë·ªÉ xe h∆°i'] = cast_to_boolean(dict_nomalized_keys['Ch·ªó ƒë·ªÉ xe h∆°i'])\n",
    "    result_dict['H∆∞·ªõng'] = cast_to_string(dict_nomalized_keys['H∆∞·ªõng'])\n",
    "    result_dict['L·ªô gi·ªõi'] = cast_to_float(dict_nomalized_keys['L·ªô gi·ªõi'].replace('m', ''))\n",
    "    result_dict['Nh√† b·∫øp'] = cast_to_boolean(dict_nomalized_keys['Nh√† b·∫øp'])\n",
    "    result_dict['Ph√°p l√Ω'] = cast_to_string(dict_nomalized_keys['Ph√°p l√Ω'])\n",
    "    result_dict['Ph√≤ng ƒÉn'] = cast_to_boolean(dict_nomalized_keys['Ph√≤ng ƒÉn'])\n",
    "    result_dict['S√¢n th∆∞·ª£ng'] = cast_to_boolean(dict_nomalized_keys['S√¢n th∆∞·ª£ng'])\n",
    "    result_dict['S·ªë l·∫ßu'] = cast_to_integer(dict_nomalized_keys['S·ªë l·∫ßu'])\n",
    "    result_dict['S·ªë ph√≤ng ng·ªß'] = cast_to_integer(dict_nomalized_keys['S·ªë ph√≤ng ng·ªß'])\n",
    "    result_dict['S·ªë toilet'] = cast_to_integer(dict_nomalized_keys['S·ªë toilet'])\n",
    "    result_dict['T·∫ßng'] = cast_to_integer(dict_nomalized_keys['T·∫ßng'])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"extra_infos\", normalize_extra_infos_dict(\"extra_infos\", lit(old_keys), lit(new_keys), lit(remove_keys)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
