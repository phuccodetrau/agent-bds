version: '3'

services:
  spark-master:
    build: .
    container_name: spark-master
    ports:
      - "8080:8080"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    networks:
      - spark-network

  spark-worker-1:
    build: .
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          memory: 1G
    networks:
      - spark-network
    depends_on:
      - spark-master

  spark-worker-2:
    build: .
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          memory: 1G
    networks:
      - spark-network
    depends_on:
      - spark-master

  # spark-worker-3:
  #   build: .
  #   container_name: spark-worker-3
  #   environment:
  #     - SPARK_MODE=worker
  #     - SPARK_MASTER_URL=spark://spark-master:7077
  #     - SPARK_WORKER_MEMORY=2G
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_RPC_AUTHENTICATION_ENABLED=no
  #     - SPARK_RPC_ENCRYPTION_ENABLED=no
  #     - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
  #     - SPARK_SSL_ENABLED=no
  #     - SPARK_USER=spark
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '1'
  #         memory: 2G
  #       reservations:
  #         memory: 1G
  #   networks:
  #     - spark-network
  #     - docker-elasticsearch_elastic
  #   depends_on:
  #     - spark-master

  # spark-submit:
  #   build: .
  #   container_name: spark-submit
  #   command: >
  #     /bin/bash -c "
  #     /opt/bitnami/spark/bin/spark-submit
  #     --master spark://spark-master:7077
  #     --jars /opt/bitnami/spark/jars/elasticsearch-spark-30_2.12-8.11.0.jar,/opt/bitnami/spark/jars/spark-sql-kafka-0-10_2.12-3.5.0.jar,/opt/bitnami/spark/jars/spark-streaming-kafka-0-10_2.12-3.5.0.jar,/opt/bitnami/spark/jars/kafka-clients-2.8.0.jar,/opt/bitnami/spark/jars/commons-pool2-2.11.1.jar,/opt/bitnami/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.0.jar,/opt/bitnami/spark/jars/commons-httpclient-3.1.jar
  #     /opt/bitnami/spark/job/stream_processing.py"
  #   volumes:
  #     - checkpoint:/tmp/checkpoint
  #   networks:
  #     - spark-network
  #   depends_on:
  #     - spark-master
  #     - spark-worker-1
  #     - spark-worker-2
  #     # - spark-worker-3

  spark-submit:
    build: .
    container_name: spark-submit
    command: >
      /bin/bash -c "
      /opt/bitnami/spark/bin/spark-submit
      --master spark://spark-master:7077
      --jars /opt/bitnami/spark/jars/gcs-connector-hadoop3-2.2.4-shaded.jar,/opt/bitnami/spark/jars/elasticsearch-spark-30_2.12-8.11.0.jar
      /opt/bitnami/spark/job/batch_processing.py"
    volumes:
      - checkpoint:/tmp/checkpoint
    networks:
      - spark-network
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
      # - spark-worker-3

networks:
  spark-network:
    driver: bridge

volumes:
  checkpoint:
    driver: local